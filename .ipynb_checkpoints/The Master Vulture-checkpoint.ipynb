{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Classifier\n",
    "\n",
    "This part builds the classifier we'll use to filter/flag our villain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "def load_file(filename,t):\n",
    "    f = open(filename,'rb')\n",
    "    lines = f.read().splitlines()\n",
    "    target = []\n",
    "    if t == \"int\":\n",
    "        for x in lines:\n",
    "            target.append(int(x))\n",
    "    else:\n",
    "        if t == \"json\":\n",
    "            for x in lines:\n",
    "                y = json.loads(x)\n",
    "                target.append(y)\n",
    "        else:\n",
    "            if t == \"arr\" or t == \"arr2\":\n",
    "                for arr in lines:\n",
    "                    #print (arr) \n",
    "                    row = arr.split(\", \")\n",
    "                    if t == \"arr2\":\n",
    "                        newr = []\n",
    "                        for c in row[0:3]:\n",
    "                            c = strip_punc(c,all=True)\n",
    "                            newr.append(c)\n",
    "                        newr.append(row[3])\n",
    "                        row = newr\n",
    "                    target.append(row)\n",
    "            else:\n",
    "                for x in lines:\n",
    "                    target.append(x)\n",
    "    return target\n",
    "collections = load_file(\"collections\",'json')\n",
    "originals = load_file(\"originals\",'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_train_data(source,label):\n",
    "    train=[]\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    i=0\n",
    "    data_size = len(source)\n",
    "    numpy.random.seed(3)\n",
    "    random_arr = numpy.random.choice(range(0,data_size),data_size)\n",
    "    train_size = round(data_size*.64,1)\n",
    "    devt_size = round(data_size*.16,1)\n",
    "    target = []\n",
    "    j = 0\n",
    "    for i in random_arr:\n",
    "        if j<train_size:\n",
    "            target=train\n",
    "        else:\n",
    "            if j<train_size+devt_size:\n",
    "                target=t1\n",
    "            else:\n",
    "                target=t2\n",
    "        pair = (source[i],label)\n",
    "        target.append(pair)\n",
    "        j=j+1\n",
    "    return (train,t1,t2)\n",
    "\n",
    "c_tt = test_train_data(collections,\"col\")\n",
    "o_tt = test_train_data(originals,\"orig\")\n",
    "\n",
    "train = c_tt[0]+o_tt[0]\n",
    "\n",
    "test1 = c_tt[1]+o_tt[1]\n",
    "test2 = c_tt[2]+o_tt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "from textblob.utils import strip_punc\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def corpus_from_text(data,ls,key):\n",
    "    #print(data)\n",
    "    if data[key] is not None:        \n",
    "        desc = strip_tags(data[key]).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            if word not in ls:\n",
    "                ls.append(word)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    stripped = (c for c in string if (0 < ord(c) < 127))\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def first_words(text,num):\n",
    "    ls = \"\"\n",
    "    j = 0\n",
    "    if text is not None:\n",
    "        desc = strip_tags(text).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            ls = ls + word\n",
    "            j = j+1\n",
    "            if j == num:\n",
    "                return ls\n",
    "        return ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_desc_corpus = ['reprints','collecting','hardcover','paperback','after','trade','as'\n",
    "                     'into','vol']\n",
    "manual_title_corpus = ['death','without','knight','four','line','by','collection','omnibus']\n",
    "desc_corpus = manual_desc_corpus\n",
    "title_corpus = manual_title_corpus\n",
    "                \n",
    "def basic_word_extractor(data,key):\n",
    "    corpus = desc_corpus\n",
    "    if key == 'name':\n",
    "        corpus = title_corpus\n",
    "    data_words = []\n",
    "    data_words = corpus_from_text(data,data_words,key)\n",
    "    features = dict((((key+'_contains({0})').format(word), (word in data_words))\n",
    "                                            for word in corpus))\n",
    "    return features\n",
    "\n",
    "def volume_features(vol_data):\n",
    "    #words_in_title = words(vol_data['name'])\n",
    "    #words_in_desc = words(vol_data['name'])\n",
    "    features= {'issue_count': vol_data['count_of_issues'],\n",
    "            'start_year': int(vol_data['start_year']),\n",
    "            'early_start': int(vol_data['start_year'])<=1990,\n",
    "            'first_word': first_words(vol_data['description'],1),\n",
    "            'first_two': first_words(vol_data['description'],2)\n",
    "            #'name_contains(by)': 'by' in vol_data['name'].lower(),\n",
    "            #'desc_contains(collected)': 'collected' in unicode(vol_data['description']).lower()\n",
    "           }\n",
    "    features.update(basic_word_extractor(vol_data,\"name\"))\n",
    "    features.update(basic_word_extractor(vol_data,\"description\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats = [(volume_features(n), category) for (n, category) in train]\n",
    "devt_feats = [(volume_features(n), category) for (n, category) in test1]\n",
    "test_feats = [(volume_features(n), category) for (n, category) in test2]\n",
    "train2_feats = [(volume_features(n), category) for (n, category) in train+test1]\n",
    "train_set = train2_feats\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n",
      "Most Informative Features\n",
      "description_contains(reprints) = True              col : orig   =     50.6 : 1.0\n",
      "description_contains(paperback) = True              col : orig   =     26.3 : 1.0\n",
      "description_contains(trade) = True              col : orig   =     16.6 : 1.0\n",
      "description_contains(collecting) = True              col : orig   =     13.1 : 1.0\n",
      "             early_start = True             orig : col    =     11.1 : 1.0\n",
      "description_contains(hardcover) = True              col : orig   =     10.1 : 1.0\n",
      "               first_two = u'seriesof'       col : orig   =      8.6 : 1.0\n",
      "              start_year = 2003              col : orig   =      8.3 : 1.0\n",
      "description_contains(vol) = True             orig : col    =      7.4 : 1.0\n",
      "              first_word = u'hardcover'      col : orig   =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = nltk.classify.accuracy(classifier, test_feats)\n",
    "print(accuracy1)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_of_label(feats,cl):\n",
    "    arr = {}\n",
    "    dist = cl.prob_classify(feats)\n",
    "    for label in dist.samples():\n",
    "        row = \"%s: %f\" % (label, dist.prob(label))\n",
    "        #print(row)\n",
    "        arr[label]=dist.prob(label)\n",
    "    return arr\n",
    "\n",
    "# EX: prob_of_label(volume_features(collections[100]),classifier)['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('col', 'orig', u'Iron Man Magazine', '\"http://comicvine.gamespot.com/iron-man-magazine/4050-32600/\"', {'col': 0.3324122319953169, 'orig': 0.6675877680046839})\n",
      "('col', 'orig', u'Marvel Masterworks: Inhumans', '\"http://comicvine.gamespot.com/marvel-masterworks-inhumans/4050-34836/\"', {'col': 0.015653535850907296, 'orig': 0.9843464641490931})\n",
      "('col', 'orig', u'Civil War: Amazing Spider-Man Decisions', '\"http://comicvine.gamespot.com/civil-war-amazing-spider-man-decisions/4050-24551/\"', {'col': 0.020124507434789444, 'orig': 0.9798754925652108})\n",
      "('orig', 'col', u'New Thunderbolts', '\"http://comicvine.gamespot.com/new-thunderbolts/4050-11298/\"', {'col': 0.586006236492409, 'orig': 0.41399376350759226})\n"
     ]
    }
   ],
   "source": [
    "def trunc_desc(vol):\n",
    "    if vol[\"description\"] is not None:\n",
    "        return vol[\"description\"][0:30]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "errors = []\n",
    "for (vol, tag) in test2:\n",
    "    #print(vol['description'])\n",
    "    if vol[\"name\"] is not None:\n",
    "        guess = classifier.classify(volume_features(vol))\n",
    "        dist = prob_of_label(volume_features(vol),classifier)\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, vol[\"name\"], json.dumps(vol['site_detail_url']), dist) )\n",
    "        \n",
    "for item in errors:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Reckoning\n",
    "\n",
    "This is where we'll put the code to:\n",
    "\n",
    "* Fetch issues from each villain\n",
    "* Check whether valid: \n",
    "    * Check i_white/i_black, v_white/v_black\n",
    "    * Check if in spiderman_ids and vol[id]==31\n",
    "    * Flag the probability it's a Trade Paperback -- make a hash table\n",
    "* Create list of the json objects for each villain's issues, periodically export to file\n",
    "* Create table from json array, limiting to the fields we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mysterio', '4333', '466', \"'http://comicvine.gamespot.com/mysterio/4005-4333/'\"]\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "true_v = load_file(\"major_villains2.txt\",\"arr2\")\n",
    "print true_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "spider_ids = load_file(\"spider_issue_ids.txt\",\"int\")\n",
    "spiderman = 1443\n",
    "random_spiders = numpy.random.choice(spider_ids,2000)\n",
    "vi_col = 1\n",
    "vn_col = 0\n",
    "\n",
    "i_black = load_file(\"i_black790\",'int')\n",
    "i_white = load_file(\"i_white790\",'int')\n",
    "v_black = load_file(\"v_black790\",'int')\n",
    "v_white = load_file(\"v_white790\",'int')\n",
    "\n",
    "resource_hash = {\"volume\":\"4050\",\"issue\":\"4000\",\"character\":\"4005\"}\n",
    "url_base = \"http://comicvine.gamespot.com/api/\"\n",
    "key = \"aff8790cd32512f45b429bb78cc21a7a87cf4d48\"\n",
    "\n",
    "actual_v_list = []\n",
    "v_issues_list = []\n",
    "\n",
    "def construct_url(i,resource):\n",
    "    new_base = url_base+resource+\"/\"+resource_hash[resource]+\"-\"+str(i)+\"/\"\n",
    "    target = new_base + \"?api_key=\"+key+\"&format=json\"\n",
    "    return target\n",
    "\n",
    "def url_to_data( url ):\n",
    "    req = urllib2.Request(url)\n",
    "    req.add_header('User-agent', 'Mozilla 5.10')\n",
    "    res = urllib2.urlopen(req)\n",
    "    data = json.load(res)['results']\n",
    "    return data\n",
    "\n",
    "def look_up(i,resource):\n",
    "    return url_to_data(construct_url(i,resource))\n",
    "\n",
    "def marvel(volume):\n",
    "    vi = volume['id']\n",
    "    if volume['publisher'] is not None:\n",
    "        if volume['publisher']['id'] == 31:\n",
    "            marvel_v.append(vi)\n",
    "            return True\n",
    "        else:\n",
    "            not_marvel.append(vi)\n",
    "            return False\n",
    "    else:\n",
    "        not_marvel.append(vi)\n",
    "        return False    \n",
    "\n",
    "def issue_list_from_vi(vi):\n",
    "    issues = look_up(vi, \"character\")[\"issue_credits\"]\n",
    "    return issues\n",
    "\n",
    "##\n",
    "def construct_issue_hash(vls):\n",
    "    v_issues_hash = {}\n",
    "    i = 0\n",
    "    for v in vls:\n",
    "        vi = v[vi_col]\n",
    "        vname = v[vn_col]\n",
    "        #print(\"adding \"+vname+\" as the \"+str(i)+\" villain\")\n",
    "        vissues = issue_list_from_vi(vi) # Why am I using name instead of id?\n",
    "        v_issues_hash[vname] = vissues\n",
    "        i = i+1\n",
    "    return v_issues_hash\n",
    "\n",
    "def p_original(ii,vol,table):\n",
    "    #Checking the white/blacklists should return over %100/0 probability -- a probability of 2 or -1 or something\n",
    "    if (ii in i_black):\n",
    "        print \"Black i\"\n",
    "        return -1\n",
    "    if (ii in i_white):\n",
    "        print \"White i\"\n",
    "        return 2\n",
    "    vi = vol[\"id\"]\n",
    "    if (vi in i_black):\n",
    "        print \"Black v\"\n",
    "        return -1\n",
    "    if (vi in i_white):\n",
    "        print \"White v\"\n",
    "        return 2\n",
    "    #Return the probability 0<p<1 this is an original\n",
    "    return p_vol(vol)\n",
    "    \n",
    "def p_vol(vol): \n",
    "    feats = volume_features(vol)\n",
    "    p = prob_of_label(feats,classifier)\n",
    "    return p[\"orig\"]\n",
    "\n",
    "def write_file(ls,filename):\n",
    "    f = open(filename, 'w')\n",
    "    for item in ls:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "def write_json_file(ls,filename):\n",
    "    f = open(filename,'w')\n",
    "    if type(ls) == dict:\n",
    "        print \"dumping dict!\"+str(len(ls))\n",
    "        d = json.dumps(ls)\n",
    "        print len(d)\n",
    "        f.write(d)\n",
    "    else:\n",
    "        json_string_arr = convert_to_star(ls)\n",
    "        for item in json_string_arr:\n",
    "            f.write(\"%s\\n\" % item)        \n",
    "        \n",
    "def convert_to_star(ls):\n",
    "    arr = []\n",
    "    for item in ls:\n",
    "        json_string = json.dumps(item)\n",
    "        arr.append(json_string)\n",
    "    return arr\n",
    "        \n",
    "def archive(ls,filen,num):\n",
    "    new_file = filen+str(num)\n",
    "    write_json_file(ls,new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marvel_v = v_white\n",
    "not_marvel = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I think I should to make a list of issue_data for each villain in this, since I'm pulling the data here     \n",
    "def process_issues(villain_iss_id_dict):\n",
    "    p_table = dict(load_file(\"p_table1500\",\"json\")[0]) #replace with whatever the latest p_table archive is\n",
    "    len(p_table)\n",
    "    name_to_issue_data = {}\n",
    "    for vill in villain_iss_id_dict:\n",
    "        i = 0\n",
    "        #vill = \"Hobgoblin Kingsley\" # Trying to see if my blacklist/whitelist does anything\n",
    "        print \"Processing \"+vill+\" \"+str(i)\n",
    "        #if i>0: #Limits to the first villain, TAKE OUT when you know it works\n",
    "            #return {\"prob\": p_table, \"name_to_issue_data\": name_to_issue_data}\n",
    "        issues = villain_iss_id_dict[vill]\n",
    "        issue_data_ls = []\n",
    "        for issue in issues: ##TAKE OUT the 0:10\n",
    "            ## ARCHIVING p_table for ever 50 additions to p_table, issue_data for every 100 issues            \n",
    "            issue_data = look_up(issue[\"id\"],\"issue\")\n",
    "            vi = issue_data[\"volume\"][\"id\"]\n",
    "            if vi in marvel_v:\n",
    "                issue_data_ls.append(issue_data) ##I am now also worried I am doing this wronng...oh I totally am, move it outsifde the if stat\n",
    "                if str(vi) not in p_table:\n",
    "                    time.sleep(1)\n",
    "                    vol_data = look_up(vi,\"volume\")\n",
    "                    prob = p_original(issue,vol_data,p_table)\n",
    "#                    print \"Adding to p_table\"\n",
    "                    p_table[vi] = prob ## add to p_table\n",
    "            else:\n",
    "                if vi not in not_marvel:\n",
    "                    time.sleep(1)\n",
    "                    vol_data = look_up(vi, \"volume\")\n",
    "                    if marvel(vol_data):\n",
    "                        issue_data_ls.append(issue_data)\n",
    "                        if str(vi) not in p_table:\n",
    "                            prob = p_original(issue,vol_data,p_table)\n",
    "#                            print \"Adding to p_table\"\n",
    "                            p_table[vi] = prob\n",
    "#                    else:\n",
    "#                        print str(vi)+\" evaluated, and not marvel!\"\n",
    "#                else:\n",
    "#                    print str(vi)+\" in the not-marvel list!\"\n",
    "                \n",
    "            i = i+1\n",
    "            time.sleep(1)\n",
    "            if (i%100 == 0) or (i == len(issues)-1):\n",
    "                print vill+str(i)\n",
    "                archive(issue_data_ls,vill,i)\n",
    "            if ((len(p_table)%50 == 0) & (len(p_table)!=1202)) or (i == len(issues)-1):\n",
    "                print \"ptable\"+str(len(p_table))\n",
    "                archive(p_table,\"p_table\",len(p_table))\n",
    "        name_to_issue_data[vill] = issue_data_ls\n",
    "        holder_dict = dict(name_to_issue_data)\n",
    "        archive(holder_dict,\"holder_dict\",0)\n",
    "        archive(marvel_v,\"marvel_v\",0)\n",
    "        archive(not_marvel,\"not_marvel\",0)\n",
    "        \n",
    "    return {\"prob\": p_table, \"name_to_issue_data\": name_to_issue_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_to_issue_ids = construct_issue_hash(true_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "test_issue_dict = dict(name_to_issue_ids)\n",
    "\n",
    "a = load_file(\"holder_dict0\",\"json\")\n",
    "b = load_file(\"holder_dict1\",\"json\")\n",
    "c = load_file(\"holder_dict2\",\"json\")\n",
    "\n",
    "a[0].update(b[0])\n",
    "a[0].update(c[0])\n",
    "\n",
    "to_pop = []\n",
    "\n",
    "for i in a[0].keys():\n",
    "    to_pop.append(str(i))\n",
    "\n",
    "done = []\n",
    "    \n",
    "for villain in to_pop:\n",
    "    if villain in test_issue_dict.keys():\n",
    "        test_issue_dict.pop(villain)\n",
    "\n",
    "print len(to_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "already_done = [\"Kraven the Hunter\", \"Scorpion\", \"Mysterio\", \"Vulture\", \"Vulture Drago\", \"Hobgoblin Kingsley\",\n",
    "             \"Hobgoblin Macendale\", \"Hobgoblin 2211\", \"Francine Frye\", \"Shocker\"]\n",
    "\n",
    "for v in already_done:\n",
    "    test_issue_dict.pop(v)\n",
    "print len(test_issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Kingpin 0\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "Kingpin100\n",
      "Kingpin200\n",
      "Kingpin300\n",
      "Kingpin400\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "Kingpin500\n",
      "Kingpin600\n",
      "Kingpin700\n",
      "Kingpin800\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "Kingpin900\n",
      "Kingpin1000\n",
      "Kingpin1100\n",
      "Kingpin1200\n",
      "Kingpin1300\n",
      "Kingpin1323\n",
      "ptable1630\n",
      "dumping dict!1630\n",
      "49020\n",
      "dumping dict!1\n",
      "18428782\n"
     ]
    }
   ],
   "source": [
    "#test_issue_dict = dict(name_to_issue_ids)\n",
    "#test_issue_dict.pop(\"Kraven the Hunter\")\n",
    "#test_issue_dict.pop(\"Scorpion\")\n",
    "#test_issue_dict.pop(\"Mysterio\")\n",
    "#test_issue_dict.pop(\"Vulture\")\n",
    "#test_issue_dict.pop(\"Vulture Drago\")\n",
    "#test_issue_dict.pop(\"Hobgoblin Kingsley\")\n",
    "#test_issue_dict.pop(\"Hobgoblin Macendale\")\n",
    "#test_issue_dict.pop(\"Hobgoblin 2211\")\n",
    "#test_issue_dict.pop(\"Francine Frye\")\n",
    "#test_issue_dict.pop(\"Shocker\")\n",
    "\n",
    "processed = process_issues(test_issue_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test 0\n",
      "22636 in the not-marvel list!\n",
      "22636 in the not-marvel list!\n",
      "43265 in the not-marvel list!\n",
      "43265 in the not-marvel list!\n",
      "56166 in the not-marvel list!\n",
      "test11\n",
      "ptable750\n",
      "dumping dict!750\n",
      "22612\n",
      "dumping dict!1\n",
      "162839\n",
      "6\n",
      "12\n",
      "7\n",
      "Original data...\n",
      "Processed data...\n",
      "Loaded file...\n",
      "set([295633, 295634, 381597, 420974, 420975])\n",
      "35\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "pre_processed_villains = process_pre_processed(already_done)\n",
    "print pre_processed_villains.keys()\n",
    "\n",
    "\n",
    "def process_pre_processed(ls):\n",
    "    pre_dict = {}\n",
    "    for villain in ls:\n",
    "        issue_data = process_pre_str(villain)\n",
    "        issue_data.append(process_last_issue(villain))\n",
    "        pre_dict[villain] = issue_data\n",
    "        \n",
    "\n",
    "def process_last_issue(v):\n",
    "    last_row = get_issue_count(v)-1\n",
    "    issue = name_to_issue_ids[v][last_row]\n",
    "    ii = issue[\"id\"]\n",
    "    i_data = look_up(ii,\"issue\")\n",
    "    vi = issue[\"volume\"][\"id\"]\n",
    "    if vi in marvel_v:\n",
    "        if str(vi) not in p_table:\n",
    "            time.sleep(1)\n",
    "            vol_data = look_up(vi,\"volume\")\n",
    "            prob = p_original(issue,vol_data,p_table)\n",
    "            p_table[vi] = prob\n",
    "        return i_data\n",
    "    else:\n",
    "        if vi not in not_marvel:\n",
    "            time.sleep(1)\n",
    "            vol_data = look_up(vi, \"volume\")\n",
    "            if marvel(vol_data):\n",
    "                if str(vi) not in p_table:\n",
    "                    prob = p_original(issue,vol_data,p_table)\n",
    "                    p_table[vi] = prob\n",
    "                return i_data\n",
    "    \n",
    "    \n",
    "def process_pre_str(v):\n",
    "    issues = load_pre_villain(v)\n",
    "    issue_data = []\n",
    "    for issue in issues:\n",
    "        issue_data.append(issue)\n",
    "    return issue_data\n",
    "\n",
    "def get_issue_count(v):\n",
    "    return len(name_to_issue_ids[v])\n",
    "    \n",
    "def load_pre_villain(v):\n",
    "    i = get_issue_count(v)\n",
    "    filename = v+str(i-1)\n",
    "    return load_file(filename,\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[0].update(pre_processed_villains)\n",
    "aggregate_iv = a[0]\n",
    "vi_to_prob = dict(load_file(\"ptable#\",\"json\")[0])\n",
    "\n",
    "nrows=0\n",
    "for villain in aggregate_iv:\n",
    "    nrows = nrows + len(aggreate_iv[villain])\n",
    "    \n",
    "print nrows\n",
    "\n",
    "probs = []\n",
    "for vol in vi_to_prob:\n",
    "    probs = vi_to_prob[vol]\n",
    "    \n",
    "print len(probs)\n",
    "print numpy.mean(probs)\n",
    "numpy.histogram(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_arr = build_csv(aggregate_issuesandvillains)\n",
    "\n",
    "\n",
    "\n",
    "def build_csv(iv_dict):\n",
    "    csv_arr = [[\"villain\", \"prob_orig\", \"issue_date\", \"issue_id\", \"volume_title\", \"volume_id\", \"volume_url\"]]\n",
    "    for vill in iv_dict:\n",
    "        issues = iv_dict[vill]\n",
    "        for issue in issues:\n",
    "            row = construct_row(vill,issue)\n",
    "            csv_arr.append(row)\n",
    "    return csv_arr\n",
    "            \n",
    "def construct_row(v_name,i_data):\n",
    "    villain = v_name\n",
    "    issue_id = i_data[\"id\"]\n",
    "    volume_id = i_data[\"volume\"][\"id\"]    \n",
    "    prob_orig = get_prob(vol_id)\n",
    "    issue_date = i_data[\"cover_date\"]\n",
    "    volume_title = i_data[\"volume\"][\"name\"]\n",
    "    volume_url = i_data[\"volume\"][\"site_detail_url\"]\n",
    "    row = [villain, prob_orig, issue_date, issue_id, volume_title, volume_id, volume_url]    \n",
    "    \n",
    "def get_prob(vi):\n",
    "    if vi in vi_to_prob:\n",
    "        return vi_to_prob[str(volume_id)]\n",
    "    else:\n",
    "        return -3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = load_file(\"test2\",\"json\")\n",
    "g = name_to_issue_ids[\"Hobgoblin 2211\"]\n",
    "\n",
    "hoohaw = process_issues({\"test\":g})\n",
    "print len(f)\n",
    "print len(g)\n",
    "print len(hoohaw[\"name_to_issue_data\"][\"test\"])\n",
    "\n",
    "original = set()\n",
    "print \"Original data...\"\n",
    "for i in g:\n",
    "    original.add(i[\"id\"])\n",
    "\n",
    "processed = set()\n",
    "print \"Processed data...\"\n",
    "for i in hoohaw[\"name_to_issue_data\"][\"test\"]:\n",
    "    processed.add(i[\"id\"])\n",
    "\n",
    "loaded = set()\n",
    "print \"Loaded file...\"\n",
    "for i in f:\n",
    "    loaded.add(i[\"id\"])\n",
    "    \n",
    "diff2 = original-processed    \n",
    "\n",
    "print diff2\n",
    "#print name_to_issue_ids[\"Hobgoblin 2211\"]\n",
    "print len(name_to_issue_ids)\n",
    "print len(test_issue_dict)\n",
    "        \n",
    "#ptable_and_vissues = process_issues(test_issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n"
     ]
    }
   ],
   "source": [
    "p = load_file(\"p_table622\",\"json\")[0]\n",
    "print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92833\n",
      "<p> Spanish trade collection of:</p><ul><li>Deadpool #1-4</li><li>Deadpool Vol. 2</li><li><a href=\"http://comicvine.gamespot.com/deadpool-kills-the-marvel-universe/4050-50940/\" data-ref-id=\"4050-50940\" rel=\"nofollow\">Deadpool Kills the Marvel Universe</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-killustrated/4050-56120/\" data-ref-id=\"4050-56120\" rel=\"nofollow\">Deadpool Killustrated</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-kills-deadpool/4050-64684/\" data-ref-id=\"4050-64684\" rel=\"nofollow\">Deadpool Kills Deadpool</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-vs-carnage/4050-72790/\" data-ref-id=\"4050-72790\" rel=\"nofollow\">Deadpool Vs. Carnage</a></li></ul><p>Published by the Spanish wing of Panini Comics.</p>\n",
      "0.44538006973\n"
     ]
    }
   ],
   "source": [
    "test_issue = name_to_issue_ids[\"Carnage\"][10]\n",
    "test_issue_data = look_up(test_issue[\"id\"],\"issue\")\n",
    "test_vi = test_issue_data[\"volume\"][\"id\"]\n",
    "test_volume = look_up(test_vi,\"volume\")\n",
    "\n",
    "p = p_original(test_issue[\"id\"],test_volume,table)\n",
    "if test_vi not in table:\n",
    "    table[test_vi] = p\n",
    "\n",
    "print test_volume[\"id\"]\n",
    "print test_volume[\"description\"]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "#God damn it I think the blacklists aren't working\n",
    "tt=[]\n",
    "\n",
    "for i in i_black[0:5]:\n",
    "    vi = look_up(i, \"issue\")[\"volume\"][\"id\"]\n",
    "    vol_data = look_up(vi, \"volume\")\n",
    "    print p_original(i,vol_data,tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{92864: 0.013352561303844828, 92833: 0.4453800697303239, 85938: 0.9995698434914074, 95207: 5.032671507280794e-09}\n"
     ]
    }
   ],
   "source": [
    "print table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
