{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Classifier\n",
    "\n",
    "This part builds the classifier we'll use to filter/flag our villain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "def load_file(filename,t):\n",
    "    f = open(filename,'rb')\n",
    "    lines = f.read().splitlines()\n",
    "    target = []\n",
    "    if t == \"int\":\n",
    "        for x in lines:\n",
    "            target.append(int(x))\n",
    "    else:\n",
    "        if t == \"json\":\n",
    "            for x in lines:\n",
    "                y = json.loads(x)\n",
    "                target.append(y)\n",
    "        else:\n",
    "            if t == \"arr\":\n",
    "                for arr in lines:\n",
    "                    #print (arr)\n",
    "                    row = arr.split(\", \")\n",
    "                    target.append(row)\n",
    "            else:\n",
    "                for x in lines:\n",
    "                    target.append(x)\n",
    "    return target\n",
    "\n",
    "i_black = load_file(\"i_black\",'int')\n",
    "i_white = load_file(\"i_white\",'int')\n",
    "v_black = load_file(\"v_black\",'int')\n",
    "v_white = load_file(\"v_white\",'int')\n",
    "collections = load_file(\"collections\",'json')\n",
    "originals = load_file(\"originals\",'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_train_data(source,label):\n",
    "    train=[]\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    i=0\n",
    "    data_size = len(source)\n",
    "    numpy.random.seed(3)\n",
    "    random_arr = numpy.random.choice(range(0,data_size),data_size)\n",
    "    train_size = round(data_size*.64,1)\n",
    "    devt_size = round(data_size*.16,1)\n",
    "    target = []\n",
    "    j = 0\n",
    "    for i in random_arr:\n",
    "        if j<train_size:\n",
    "            target=train\n",
    "        else:\n",
    "            if j<train_size+devt_size:\n",
    "                target=t1\n",
    "            else:\n",
    "                target=t2\n",
    "        pair = (source[i],label)\n",
    "        target.append(pair)\n",
    "        j=j+1\n",
    "    return (train,t1,t2)\n",
    "\n",
    "c_tt = test_train_data(collections,\"col\")\n",
    "o_tt = test_train_data(originals,\"orig\")\n",
    "\n",
    "train = c_tt[0]+o_tt[0]\n",
    "\n",
    "test1 = c_tt[1]+o_tt[1]\n",
    "test2 = c_tt[2]+o_tt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "from textblob.utils import strip_punc\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def corpus_from_text(data,ls,key):\n",
    "    #print(data)\n",
    "    if data[key] is not None:        \n",
    "        desc = strip_tags(data[key]).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            if word not in ls:\n",
    "                ls.append(word)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    stripped = (c for c in string if (0 < ord(c) < 127))\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def first_words(text,num):\n",
    "    ls = \"\"\n",
    "    j = 0\n",
    "    if text is not None:\n",
    "        desc = strip_tags(text).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            ls = ls + word\n",
    "            j = j+1\n",
    "            if j == num:\n",
    "                return ls\n",
    "        return ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_desc_corpus = ['reprints','collecting','hardcover','paperback','after','trade','as'\n",
    "                     'into','vol']\n",
    "manual_title_corpus = ['death','without','knight','four','line','by','collection','omnibus']\n",
    "desc_corpus = manual_desc_corpus\n",
    "title_corpus = manual_title_corpus\n",
    "                \n",
    "def basic_word_extractor(data,key):\n",
    "    corpus = desc_corpus\n",
    "    if key == 'name':\n",
    "        corpus = title_corpus\n",
    "    data_words = []\n",
    "    data_words = corpus_from_text(data,data_words,key)\n",
    "    features = dict((((key+'_contains({0})').format(word), (word in data_words))\n",
    "                                            for word in corpus))\n",
    "    return features\n",
    "\n",
    "def volume_features(vol_data):\n",
    "    #words_in_title = words(vol_data['name'])\n",
    "    #words_in_desc = words(vol_data['name'])\n",
    "    features= {'issue_count': vol_data['count_of_issues'],\n",
    "            'start_year': int(vol_data['start_year']),\n",
    "            'early_start': int(vol_data['start_year'])<=1990,\n",
    "            'first_word': first_words(vol_data['description'],1),\n",
    "            'first_two': first_words(vol_data['description'],2)\n",
    "            #'name_contains(by)': 'by' in vol_data['name'].lower(),\n",
    "            #'desc_contains(collected)': 'collected' in unicode(vol_data['description']).lower()\n",
    "           }\n",
    "    features.update(basic_word_extractor(vol_data,\"name\"))\n",
    "    features.update(basic_word_extractor(vol_data,\"description\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats = [(volume_features(n), category) for (n, category) in train]\n",
    "devt_feats = [(volume_features(n), category) for (n, category) in test1]\n",
    "test_feats = [(volume_features(n), category) for (n, category) in test2]\n",
    "train2_feats = [(volume_features(n), category) for (n, category) in train+test1]\n",
    "train_set = train2_feats\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n",
      "Most Informative Features\n",
      "description_contains(reprints) = True              col : orig   =     50.6 : 1.0\n",
      "description_contains(paperback) = True              col : orig   =     26.3 : 1.0\n",
      "description_contains(trade) = True              col : orig   =     16.6 : 1.0\n",
      "description_contains(collecting) = True              col : orig   =     13.1 : 1.0\n",
      "             early_start = True             orig : col    =     11.1 : 1.0\n",
      "description_contains(hardcover) = True              col : orig   =     10.1 : 1.0\n",
      "               first_two = u'seriesof'       col : orig   =      8.6 : 1.0\n",
      "              start_year = 2003              col : orig   =      8.3 : 1.0\n",
      "description_contains(vol) = True             orig : col    =      7.4 : 1.0\n",
      "              first_word = u'hardcover'      col : orig   =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = nltk.classify.accuracy(classifier, test_feats)\n",
    "print(accuracy1)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_of_label(feats,cl):\n",
    "    arr = {}\n",
    "    dist = cl.prob_classify(feats)\n",
    "    for label in dist.samples():\n",
    "        row = \"%s: %f\" % (label, dist.prob(label))\n",
    "        #print(row)\n",
    "        arr[label]=dist.prob(label)\n",
    "    return arr\n",
    "\n",
    "# EX: prob_of_label(volume_features(collections[100]),classifier)['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('col', 'orig', u'Iron Man Magazine', '\"http://comicvine.gamespot.com/iron-man-magazine/4050-32600/\"', {'col': 0.3324122319953169, 'orig': 0.6675877680046839})\n",
      "('col', 'orig', u'Marvel Masterworks: Inhumans', '\"http://comicvine.gamespot.com/marvel-masterworks-inhumans/4050-34836/\"', {'col': 0.015653535850907296, 'orig': 0.9843464641490931})\n",
      "('col', 'orig', u'Civil War: Amazing Spider-Man Decisions', '\"http://comicvine.gamespot.com/civil-war-amazing-spider-man-decisions/4050-24551/\"', {'col': 0.020124507434789444, 'orig': 0.9798754925652108})\n",
      "('orig', 'col', u'New Thunderbolts', '\"http://comicvine.gamespot.com/new-thunderbolts/4050-11298/\"', {'col': 0.586006236492409, 'orig': 0.41399376350759226})\n"
     ]
    }
   ],
   "source": [
    "def trunc_desc(vol):\n",
    "    if vol[\"description\"] is not None:\n",
    "        return vol[\"description\"][0:30]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "errors = []\n",
    "for (vol, tag) in test2:\n",
    "    #print(vol['description'])\n",
    "    if vol[\"name\"] is not None:\n",
    "        guess = classifier.classify(volume_features(vol))\n",
    "        dist = prob_of_label(volume_features(vol),classifier)\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, vol[\"name\"], json.dumps(vol['site_detail_url']), dist) )\n",
    "        \n",
    "for item in errors:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Reckoning\n",
    "\n",
    "This is where we'll put the code to:\n",
    "\n",
    "* Fetch issues from each villain\n",
    "* Check whether valid: \n",
    "    * Check i_white/i_black, v_white/v_black\n",
    "    * Check if in spiderman_ids and vol[id]==31\n",
    "    * Flag the probability it's a Trade Paperback -- make a hash table\n",
    "* Create list of the json objects for each villain's issue, periodically export to file\n",
    "* Create table from json array, limiting to the fields we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['name', 'id', 'count_of_issue_appearances', 'site_url'], ['Mysterio', '4333', '466', 'http://comicvine.gamespot.com/mysterio/4005-4333/'], ['Mysterio (Berkhart)', '84871', '43', 'http://comicvine.gamespot.com/mysterio-berkhart/4005-84871/'], ['Francis Klum', '84872', '21', 'http://comicvine.gamespot.com/francis-klum/4005-84872/'], ['Hobgoblin (Kingsley)', '7605', '317', 'http://comicvine.gamespot.com/hobgoblin-kingsley/4005-7605/']]\n"
     ]
    }
   ],
   "source": [
    "v_candidates = load_file(\"villains.txt\",\"arr\")\n",
    "print v_candidates[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spider_ids = load_file(\"spider_issue_ids.txt\",\"int\")\n",
    "spiderman = 1443\n",
    "random_spiders = numpy.random.choice(spider_ids,2000)\n",
    "\n",
    "i_black = load_file(\"i_black\",'int')\n",
    "i_white = load_file(\"i_white\",'int')\n",
    "v_black = load_file(\"v_black\",'int')\n",
    "v_white = load_file(\"v_white\",'int')\n",
    "\n",
    "resource_hash = {\"volume\":\"4050\",\"issue\":\"4000\",\"character\":\"4005\"}\n",
    "url_base = \"http://comicvine.gamespot.com/api/\"\n",
    "key = \"aff8790cd32512f45b429bb78cc21a7a87cf4d48\"\n",
    "\n",
    "actual_v_list = []\n",
    "v_issues_list = []\n",
    "\n",
    "def construct_url(i,resource):\n",
    "    new_base = url_base+resource+\"/\"+resource_hash[resource]+\"-\"+str(i)+\"/\"\n",
    "    target = new_base + \"?api_key=\"+key+\"&format=json\"\n",
    "    return target\n",
    "\n",
    "def url_to_data( url ):\n",
    "    req = urllib2.Request(url)\n",
    "    req.add_header('User-agent', 'Mozilla 5.10')\n",
    "    res = urllib2.urlopen(req)\n",
    "    data = json.load(res)['results']\n",
    "    return data\n",
    "\n",
    "def look_up(i,resource):\n",
    "    return url_to_data(construct_url(i,resource))\n",
    "\n",
    "def is_enemy(char_data):\n",
    "    enemies = char_data['character_enemies']\n",
    "    if enemies is not None:\n",
    "        for e in enemies:\n",
    "            if e['id'] == spiderman:\n",
    "                return True\n",
    "        return False\n",
    "    return False            \n",
    "\n",
    "def marvel(vol_id):\n",
    "    data = look_up(vol_id,\"volume\") #API Call\n",
    "    if data['publisher']['id'] == 31:\n",
    "        return True\n",
    "\n",
    "def process_issues(ids):\n",
    "    data = []\n",
    "    for i in ids:\n",
    "        if i not in i_black:\n",
    "            issue_data = look_up(i,\"issue\") #API Call\n",
    "            if i in i_white:\n",
    "                data.append((issue_data,2))\n",
    "            else:\n",
    "                v = issue_data['volume']\n",
    "                if v['id'] not in v_black:\n",
    "                    if v['id'] in i_white:\n",
    "                        data.append((issue_data,2))\n",
    "                    else:\n",
    "                        if marvel(v['id']):\n",
    "                            p_dist = prob_of_label(volume_features(v),classifier)\n",
    "                            p_col = p_dist['col']\n",
    "                            data.append((issue_data,p_col))\n",
    "    return data                \n",
    "\n",
    "def construct_villain_issue_list():\n",
    "    j = 0\n",
    "    for row in v_candidates:\n",
    "        if j != 0:\n",
    "            vi = row[1]\n",
    "            data = look_up(vi,\"character\")\n",
    "            if is_enemy(data):\n",
    "                actual_v_list.append(vi)\n",
    "                issue_ids = []\n",
    "                for issue in data['issue_credits']:\n",
    "                    issue_ids.append(issue['id'])\n",
    "                v_issues_list = v_issues_list+process_issues(issue_ids)\n",
    "                #archive(\"vill_json\", v_issues_list)\n",
    "        j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://comicvine.gamespot.com/api/volume/4050-77941/?api_key=aff8790cd32512f45b429bb78cc21a7a87cf4d48&format=json\n"
     ]
    }
   ],
   "source": [
    "villain_issue_data = construct_villain_issue_list()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
