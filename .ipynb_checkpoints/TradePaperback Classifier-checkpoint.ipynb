{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "\n",
    "### Make a filterless set\n",
    "Use the script to make a smaller parallel set out of the same issue ids without the field filters, and see if it does better with a small-enough-not-to-be-a-pain amount of data. Load all the volumes to the preset lists.\n",
    "\n",
    "### Export better, for subsetting\n",
    "Figure out how to re-json the data (I think it's 'dump' or 'dumps') and then you don't need the smaller dataset, and can maybe do cleaner analysis\n",
    "\n",
    "### Find a better ML library?\n",
    "Something that lets me treat numbers AND text as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib2\n",
    "import json\n",
    "numpy.random.seed(0)\n",
    "spider_ids = [int(line.rstrip('\\n')) for line in open('spider_issue_ids.txt')]\n",
    "random_spiders = numpy.random.choice(spider_ids,2000)\n",
    "\n",
    "i_black = []\n",
    "i_white = []\n",
    "\n",
    "v_white = []\n",
    "v_black = []\n",
    "\n",
    "collections = []\n",
    "originals = []\n",
    "\n",
    "def load_file(filename,t):\n",
    "    f = open(filename,'rb')\n",
    "    lines = f.read().splitlines()\n",
    "    target = []\n",
    "    if t == \"int\":\n",
    "        for x in lines:\n",
    "            target.append(int(x))\n",
    "    return target\n",
    "\n",
    "resource_hash = {'volume':4050,'issue':4000}\n",
    "\n",
    "key = \"aff8790cd32512f45b429bb78cc21a7a87cf4d48\"\n",
    "url_base = \"http://comicvine.gamespot.com/api/\"\n",
    "v_fields = [\"name\",\"id\",\"description\",\"count_of_issues\",\"start_year\",\"publisher\",\"site_detail_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preset_b = [27474,18946,22645,22769,23235,23306,23402,26570,32701,34138,48343,23632,\n",
    "#              25702,1283,2293,3490]\n",
    "#preset_w = [85928,2127,73420,22146,22405,22719,18354,75557,85312,78355,2576,2863,6011,18024,\n",
    "#           7515,5561,2686,2870,3012,11069,3519,28576,4421,9142,5048,10760,11306,5562,\n",
    "#           5788,18177,9375,7255,11066,11497,33777,17998,18459,2128,2045,43884,6218]\n",
    "\n",
    "preset_b = load_file(\"v_black\",\"int\")\n",
    "preset_w = load_file(\"v_white\",\"int\")\n",
    "\n",
    "collected_names = [\"mcfarlane\",\"stern\",\"marvel masterworks\", \"marvel-masterworks\", \"omnibus\", \n",
    "                   \"essential\", \"epic\", \"collection\",\"collected\",\"trade paperback\",\n",
    "                   \"hard cover\",\"hardcover\",\"tp/hc\", \"complete\", \"soft cover\", \"softcover\"]\n",
    "\n",
    "#resampling -- 2000 ids gave us 15 collections and 35 originals\n",
    "numpy.random.seed(0)\n",
    "random_spiders = numpy.random.choice(spider_ids,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_url(i,resource_str,fields=[]):\n",
    "    resource_n = resource_hash[resource_str]\n",
    "    url = url_base+resource_str+\"/\"+str(resource_n)+\"-\"+str(i)+\"/?format=json&api_key=\"+key\n",
    "#    print url\n",
    "    if fields !=[]:\n",
    "        url = add_fields(url,fields)\n",
    "    return url\n",
    "\n",
    "def add_fields(base,f=[]):\n",
    "    url = base + \"&field_list=\"\n",
    "    j = 0\n",
    "    for item in f:\n",
    "        url = url+str(item)\n",
    "        if j < len(f)-1:\n",
    "            url = url+\",\"\n",
    "        j = j+1\n",
    "    return url\n",
    "                \n",
    "def url_to_data(url):\n",
    "    req = urllib2.Request(url)\n",
    "    req.add_header('User-agent', 'Mozilla 5.10')\n",
    "    res = urllib2.urlopen(req)\n",
    "    data = json.load(res)\n",
    "    return data\n",
    "\n",
    "def check_marvel(v):\n",
    "    pub = v['publisher']\n",
    "    if pub is None:\n",
    "        return False\n",
    "    if pub['id'] == 31:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def blacklist(idat, vdat):\n",
    "    i_black.append(idat['id'])\n",
    "    v_black.append(vdat['id'])\n",
    "    collections.append(vdat)\n",
    "    \n",
    "def whitelist(idat, vdat):\n",
    "    i_white.append(idat['id'])\n",
    "    v_white.append(vdat['id'])\n",
    "    originals.append(vdat)\n",
    "\n",
    "def keep_track_of_length(ls):\n",
    "    if len(ls)%10 == 0:\n",
    "        if ls == collections:\n",
    "            print(\"collections\")\n",
    "        if ls == originals:\n",
    "            print(\"originals\")\n",
    "        print(len(ls))\n",
    "    \n",
    "def test_id(i):\n",
    "    if (i in i_black) or (i in i_white):\n",
    "        print(\"known issue\")\n",
    "        return\n",
    "    issue_data = url_to_data(construct_url(i,'issue'))['results']\n",
    "#    print(issue_data)\n",
    "    title = issue_data['volume']['name'].lower()\n",
    "    vi = issue_data['volume']['id']\n",
    "    if (vi in v_white) or (vi in v_black):\n",
    "        print(\"known volume\")\n",
    "        return\n",
    "    vol_data = url_to_data(construct_url(vi,'volume'))['results']#,v_fields))['results']\n",
    "    if (check_presets(issue_data,vol_data)):\n",
    "        print(\"preset volume\")\n",
    "        return\n",
    "    if (not check_marvel(vol_data)):        \n",
    "        print(\"not marvel\")\n",
    "        return\n",
    "    for term in collected_names:\n",
    "        if term in title:\n",
    "            blacklist(issue_data, vol_data)\n",
    "            print(\"keyword filter: \"+title)\n",
    "            return\n",
    "    if(ask_if_collection(vol_data['site_detail_url'])):\n",
    "#        print(str(issue_data['id'])+','+str(vol_data['id']))\n",
    "        blacklist(issue_data, vol_data)\n",
    "    else:\n",
    "#        print(str(issue_data['id'])+','+str(vol_data['id']))\n",
    "        whitelist(issue_data, vol_data)\n",
    "    keep_track_of_length(collections)\n",
    "    keep_track_of_length(originals)\n",
    "    return\n",
    "    \n",
    "def test_id_list(ls=[]):\n",
    "    j = 0\n",
    "    for item in ls:\n",
    "        test_id(item)\n",
    "        if (len(ls)-j)%100==0:\n",
    "            print str(len(ls)-j)+\" ids left to go!\"\n",
    "        j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to fast-track whitelist the preset vols -- when they come up in the random crawl\n",
    "\n",
    "def check_presets(idat,vdat):\n",
    "    if vdat['id'] in preset_w:\n",
    "        whitelist(idat,vdat)\n",
    "        return True\n",
    "    if vdat['id'] in preset_b:\n",
    "        blacklist(idat,vdat)\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35917, 43901, 21748, 55746, 35188]\n",
      "[36364, 2127, 18619, 6354, 6218]\n",
      "known issue\n"
     ]
    }
   ],
   "source": [
    "print preset_b[0:5]\n",
    "print preset_w[0:5]\n",
    "\n",
    "test_id_list(random_spiders[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_file(ls,filename):\n",
    "    f = open(filename, 'w')\n",
    "    for item in ls:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "#write_file(collections,\"collections\")\n",
    "#write_file(originals,\"originals\")\n",
    "#write_file(i_black,\"i_black\")\n",
    "#write_file(i_white,\"i_white\")\n",
    "#write_file(v_black,\"v_black\")\n",
    "#write_file(v_white,\"v_white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ask_if_collection(link):\n",
    "    print link\n",
    "    while True:\n",
    "        answer = raw_input()\n",
    "        if answer == 'c':\n",
    "            return True\n",
    "        if answer == 'o':\n",
    "            return False\n",
    "        print(\"Enter 'c' for collection, 'o' for original:\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
