{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "def load_file(filename,t):\n",
    "    f = open(filename,'rb')\n",
    "    lines = f.read().splitlines()\n",
    "    target = []\n",
    "    if t == \"int\":\n",
    "        for x in lines:\n",
    "            target.append(int(x))\n",
    "    else:\n",
    "        if t == \"json\":\n",
    "            for x in lines:\n",
    "                y = json.loads(x)\n",
    "                target.append(y)\n",
    "        else:\n",
    "            for x in lines:\n",
    "                target.append(x)\n",
    "    return target\n",
    "\n",
    "collections = load_file(\"collections\",'json')\n",
    "originals = load_file(\"originals\",'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_train_data(source,label):\n",
    "    train=[]\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    i=0\n",
    "    data_size = len(source)\n",
    "    numpy.random.seed(3)\n",
    "    random_arr = numpy.random.choice(range(0,data_size),data_size)\n",
    "    train_size = round(data_size*.64,1)\n",
    "    devt_size = round(data_size*.16,1)\n",
    "    target = []\n",
    "    j = 0\n",
    "    for i in random_arr:\n",
    "        if j<train_size:\n",
    "            target=train\n",
    "        else:\n",
    "            if j<train_size+devt_size:\n",
    "                target=t1\n",
    "            else:\n",
    "                target=t2\n",
    "        pair = (source[i],label)\n",
    "        target.append(pair)\n",
    "        j=j+1\n",
    "    return (train,t1,t2)\n",
    "\n",
    "c_tt = test_train_data(collections,\"col\")\n",
    "o_tt = test_train_data(originals,\"orig\")\n",
    "\n",
    "train = c_tt[0]+o_tt[0]\n",
    "\n",
    "test1 = c_tt[1]+o_tt[1]\n",
    "test2 = c_tt[2]+o_tt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "from textblob.utils import strip_punc\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def corpus_from_text(data,ls,key):\n",
    "    #print(data)\n",
    "    if data[key] is not None:        \n",
    "        desc = strip_tags(data[key]).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            if word not in ls:\n",
    "                ls.append(word)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    stripped = (c for c in string if (0 < ord(c) < 127))\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def first_words(text,num):\n",
    "    ls = \"\"\n",
    "    j = 0\n",
    "    if text is not None:\n",
    "        desc = strip_tags(text).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            ls = ls + word\n",
    "            j = j+1\n",
    "            if j == num:\n",
    "                return ls\n",
    "        return ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_desc_corpus = ['reprints','collecting','hardcover','paperback','after','trade','as'\n",
    "                     'into','vol']\n",
    "manual_title_corpus = ['death','without','knight','four','line','by','collection','omnibus']\n",
    "desc_corpus = manual_desc_corpus\n",
    "title_corpus = manual_title_corpus\n",
    "                \n",
    "def basic_word_extractor(data,key):\n",
    "    corpus = desc_corpus\n",
    "    if key == 'name':\n",
    "        corpus = title_corpus\n",
    "    data_words = []\n",
    "    data_words = corpus_from_text(data,data_words,key)\n",
    "    features = dict((((key+'_contains({0})').format(word), (word in data_words))\n",
    "                                            for word in corpus))\n",
    "    return features\n",
    "\n",
    "def volume_features(vol_data):\n",
    "    #words_in_title = words(vol_data['name'])\n",
    "    #words_in_desc = words(vol_data['name'])\n",
    "    features= {'issue_count': vol_data['count_of_issues'],\n",
    "            'start_year': int(vol_data['start_year']),\n",
    "            'early_start': int(vol_data['start_year'])<=1990,\n",
    "            'first_word': first_words(vol_data['description'],1),\n",
    "            'first_two': first_words(vol_data['description'],2)\n",
    "            #'name_contains(by)': 'by' in vol_data['name'].lower(),\n",
    "            #'desc_contains(collected)': 'collected' in unicode(vol_data['description']).lower()\n",
    "           }\n",
    "    features.update(basic_word_extractor(vol_data,\"name\"))\n",
    "    features.update(basic_word_extractor(vol_data,\"description\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats = [(volume_features(n), category) for (n, category) in train]\n",
    "devt_feats = [(volume_features(n), category) for (n, category) in test1]\n",
    "test_feats = [(volume_features(n), category) for (n, category) in test2]\n",
    "train2_feats = [(volume_features(n), category) for (n, category) in train+test1]\n",
    "train_set = train2_feats\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947368421053\n",
      "Most Informative Features\n",
      "description_contains(reprints) = True              col : orig   =     50.6 : 1.0\n",
      "description_contains(paperback) = True              col : orig   =     26.3 : 1.0\n",
      "description_contains(trade) = True              col : orig   =     16.6 : 1.0\n",
      "description_contains(collecting) = True              col : orig   =     13.1 : 1.0\n",
      "             early_start = True             orig : col    =     11.1 : 1.0\n",
      "description_contains(hardcover) = True              col : orig   =     10.1 : 1.0\n",
      "               first_two = u'seriesof'       col : orig   =      8.6 : 1.0\n",
      "              start_year = 2003              col : orig   =      8.3 : 1.0\n",
      "description_contains(vol) = True             orig : col    =      7.4 : 1.0\n",
      "              first_word = u'hardcover'      col : orig   =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = nltk.classify.accuracy(classifier, test_feats)\n",
    "print(accuracy1)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_of_label(feats,cl):\n",
    "    arr = {}\n",
    "    dist = cl.prob_classify(feats)\n",
    "    for label in dist.samples():\n",
    "        row = \"%s: %f\" % (label, dist.prob(label))\n",
    "        #print(row)\n",
    "        arr[label]=dist.prob(label)\n",
    "    return arr\n",
    "\n",
    "## EX: prob_of_label(volume_features(collections[100]),classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('col', 'orig', u'Iron Man Magazine', '\"http://comicvine.gamespot.com/iron-man-magazine/4050-32600/\"', {'col': 0.3324122319953169, 'orig': 0.6675877680046839})\n",
      "('col', 'orig', u'Marvel Masterworks: Inhumans', '\"http://comicvine.gamespot.com/marvel-masterworks-inhumans/4050-34836/\"', {'col': 0.015653535850907296, 'orig': 0.9843464641490931})\n",
      "('col', 'orig', u'Civil War: Amazing Spider-Man Decisions', '\"http://comicvine.gamespot.com/civil-war-amazing-spider-man-decisions/4050-24551/\"', {'col': 0.020124507434789444, 'orig': 0.9798754925652108})\n",
      "('orig', 'col', u'New Thunderbolts', '\"http://comicvine.gamespot.com/new-thunderbolts/4050-11298/\"', {'col': 0.586006236492409, 'orig': 0.41399376350759226})\n"
     ]
    }
   ],
   "source": [
    "def trunc_desc(vol):\n",
    "    if vol[\"description\"] is not None:\n",
    "        return vol[\"description\"][0:30]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "errors = []\n",
    "for (vol, tag) in test2:\n",
    "    #print(vol['description'])\n",
    "    if vol[\"name\"] is not None:\n",
    "        guess = classifier.classify(volume_features(vol))\n",
    "        dist = prob_of_label(volume_features(vol),classifier)\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, vol[\"name\"], json.dumps(vol['site_detail_url']), dist) )\n",
    "        \n",
    "for item in errors:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
