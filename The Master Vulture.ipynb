{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Classifier\n",
    "\n",
    "This part builds the classifier we'll use to filter/flag our villain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import numpy\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "def load_file(filename,t):\n",
    "    f = open(filename,'rb')\n",
    "    lines = f.read().splitlines()\n",
    "    target = []\n",
    "    if t == \"int\":\n",
    "        for x in lines:\n",
    "            target.append(int(x))\n",
    "    else:\n",
    "        if t == \"json\":\n",
    "            for x in lines:\n",
    "                y = json.loads(x)\n",
    "                target.append(y)\n",
    "        else:\n",
    "            if t == \"arr\" or t == \"arr2\":\n",
    "                for arr in lines:\n",
    "                    #print (arr) \n",
    "                    row = arr.split(\", \")\n",
    "                    if t == \"arr2\":\n",
    "                        newr = []\n",
    "                        for c in row[0:3]:\n",
    "                            c = strip_punc(c,all=True)\n",
    "                            newr.append(c)\n",
    "                        newr.append(row[3])\n",
    "                        row = newr\n",
    "                    target.append(row)\n",
    "            else:\n",
    "                for x in lines:\n",
    "                    target.append(x)\n",
    "    return target\n",
    "collections = load_file(\"collections\",'json')\n",
    "originals = load_file(\"originals\",'json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_train_data(source,label):\n",
    "    train=[]\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    i=0\n",
    "    data_size = len(source)\n",
    "    numpy.random.seed(3)\n",
    "    random_arr = numpy.random.choice(range(0,data_size),data_size)\n",
    "    train_size = round(data_size*.64,1)\n",
    "    devt_size = round(data_size*.16,1)\n",
    "    target = []\n",
    "    j = 0\n",
    "    for i in random_arr:\n",
    "        if j<train_size:\n",
    "            target=train\n",
    "        else:\n",
    "            if j<train_size+devt_size:\n",
    "                target=t1\n",
    "            else:\n",
    "                target=t2\n",
    "        pair = (source[i],label)\n",
    "        target.append(pair)\n",
    "        j=j+1\n",
    "    return (train,t1,t2)\n",
    "\n",
    "c_tt = test_train_data(collections,\"col\")\n",
    "o_tt = test_train_data(originals,\"orig\")\n",
    "\n",
    "train = c_tt[0]+o_tt[0]\n",
    "\n",
    "test1 = c_tt[1]+o_tt[1]\n",
    "test2 = c_tt[2]+o_tt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "import string\n",
    "from textblob.utils import strip_punc\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def corpus_from_text(data,ls,key):\n",
    "    #print(data)\n",
    "    if data[key] is not None:        \n",
    "        desc = strip_tags(data[key]).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            if word not in ls:\n",
    "                ls.append(word)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip_non_ascii(string):\n",
    "    stripped = (c for c in string if (0 < ord(c) < 127))\n",
    "    return ''.join(stripped)\n",
    "\n",
    "def first_words(text,num):\n",
    "    ls = \"\"\n",
    "    j = 0\n",
    "    if text is not None:\n",
    "        desc = strip_tags(text).lower()\n",
    "        desc = desc.split()\n",
    "        for word in desc:\n",
    "            word = strip_non_ascii(word)\n",
    "            word = strip_punc(word,all=True)\n",
    "            #print(word)\n",
    "            ls = ls + word\n",
    "            j = j+1\n",
    "            if j == num:\n",
    "                return ls\n",
    "        return ls\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manual_desc_corpus = ['reprints','collection','collects','collected','collecting','hardcover','paperback','after','trade','as'\n",
    "                     'into','vol']\n",
    "manual_title_corpus = ['masterworks','death','without','knight','four','line','by','collection','omnibus']\n",
    "desc_corpus = manual_desc_corpus\n",
    "title_corpus = manual_title_corpus\n",
    "                \n",
    "def basic_word_extractor(data,key):\n",
    "    corpus = desc_corpus\n",
    "    if key == 'name':\n",
    "        corpus = title_corpus\n",
    "    data_words = []\n",
    "    data_words = corpus_from_text(data,data_words,key)\n",
    "    features = dict((((key+'_contains({0})').format(word), (word in data_words))\n",
    "                                            for word in corpus))\n",
    "    return features\n",
    "\n",
    "def volume_features(vol_data):\n",
    "    #words_in_title = words(vol_data['name'])\n",
    "    #words_in_desc = words(vol_data['name'])\n",
    "    features= {'issue_count': vol_data['count_of_issues'],\n",
    "            'start_year': int(vol_data['start_year']),\n",
    "            'early_start': int(vol_data['start_year'])<=1990,\n",
    "            'first_word': first_words(vol_data['description'],1),\n",
    "            'first_two': first_words(vol_data['description'],2),\n",
    "            'name_contains(masterworks)': 'masterworks' in vol_data['name'].lower(),\n",
    "            'name_contains(by)': 'by' in vol_data['name'].lower(),\n",
    "            'name_contains(omnibus)': 'omnibus' in vol_data['name'].lower(),\n",
    "            'desc_contains(collection)': 'collection' in vol_data['name'].lower(),\n",
    "            'desc_contains(collected)': 'collected' in unicode(vol_data['description']).lower()\n",
    "           }\n",
    "    features.update(basic_word_extractor(vol_data,\"name\"))\n",
    "    features.update(basic_word_extractor(vol_data,\"description\"))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats = [(volume_features(n), category) for (n, category) in train]\n",
    "devt_feats = [(volume_features(n), category) for (n, category) in test1]\n",
    "test_feats = [(volume_features(n), category) for (n, category) in test2]\n",
    "train2_feats = [(volume_features(n), category) for (n, category) in train+test1]\n",
    "train_set = train2_feats\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986842105263\n",
      "Most Informative Features\n",
      "description_contains(reprints) = True              col : orig   =     50.6 : 1.0\n",
      "description_contains(paperback) = True              col : orig   =     26.3 : 1.0\n",
      "description_contains(trade) = True              col : orig   =     16.6 : 1.0\n",
      "desc_contains(collected) = True             orig : col    =     15.5 : 1.0\n",
      "description_contains(collecting) = True              col : orig   =     13.1 : 1.0\n",
      "description_contains(collection) = True             orig : col    =     12.1 : 1.0\n",
      "             early_start = True             orig : col    =     11.1 : 1.0\n",
      "description_contains(hardcover) = True              col : orig   =     10.1 : 1.0\n",
      "               first_two = u'seriesof'       col : orig   =      8.6 : 1.0\n",
      "              start_year = 2003              col : orig   =      8.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = nltk.classify.accuracy(classifier, test_feats)\n",
    "print(accuracy1)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_of_label(feats,cl):\n",
    "    arr = {}\n",
    "    dist = cl.prob_classify(feats)\n",
    "    for label in dist.samples():\n",
    "        row = \"%s: %f\" % (label, dist.prob(label))\n",
    "        #print(row)\n",
    "        arr[label]=dist.prob(label)\n",
    "    return arr\n",
    "\n",
    "# EX: prob_of_label(volume_features(collections[100]),classifier)['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('col', 'orig', u'Civil War: Amazing Spider-Man Decisions', '\"http://comicvine.gamespot.com/civil-war-amazing-spider-man-decisions/4050-24551/\"', {'col': 0.054193301267108016, 'orig': 0.9458066987328917})\n",
      "('col', 'orig', u'Spider-Man Classics', '\"http://comicvine.gamespot.com/spider-man-classics/4050-5047/\"', {'col': 0.03550539663835564, 'orig': 0.9644946033616447})\n",
      "('orig', 'col', u'The Unbeatable Squirrel Girl Beats Up the Marvel Universe', '\"http://comicvine.gamespot.com/the-unbeatable-squirrel-girl-beats-up-the-marvel-u/4050-94668/\"', {'col': 0.9909133043201238, 'orig': 0.009086695679875013})\n"
     ]
    }
   ],
   "source": [
    "def trunc_desc(vol):\n",
    "    if vol[\"description\"] is not None:\n",
    "        return vol[\"description\"][0:30]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "errors = []\n",
    "for (vol, tag) in test2+test1:\n",
    "    #print(vol['description'])\n",
    "    if vol[\"name\"] is not None:\n",
    "        guess = classifier.classify(volume_features(vol))\n",
    "        dist = prob_of_label(volume_features(vol),classifier)\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, vol[\"name\"], json.dumps(vol['site_detail_url']), dist) )\n",
    "        \n",
    "for item in errors:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'i_black' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-15ff9fff2f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mp_table_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_p_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-15ff9fff2f71>\u001b[0m in \u001b[0;36mupdate_p_table\u001b[0;34m(prob_dict)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnew_table\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprob_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi_black\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Black v\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mnew_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'i_black' is not defined"
     ]
    }
   ],
   "source": [
    "p_table=load_file(\"with_preloads0\",\"json\")[0]\n",
    "\n",
    "def update_p_table(prob_dict):\n",
    "    i = 0\n",
    "    new_table={}\n",
    "    for vol in prob_dict:\n",
    "        if (vol in i_black):\n",
    "            print \"Black v\"\n",
    "            new_table[vol]=-1\n",
    "        else:\n",
    "            if (vol in i_white):\n",
    "                print \"White v\"\n",
    "                new_table[vol]=2\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "                v_data=look_up(vol,\"volume\")\n",
    "        #print v_data[\"count_of_issues\"]\n",
    "                new_table[vol]=p_vol(v_data)\n",
    "        i=i+1\n",
    "        if (i%100 == 0):\n",
    "            print len(new_table)\n",
    "    return new_table   \n",
    "    #Return the probability 0<p<1 this is an original\n",
    "    \n",
    "\n",
    "p_table_2 = update_p_table(p_table) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping dict!1630\n",
      "49226\n"
     ]
    }
   ],
   "source": [
    "archive(p_table_2,\"juked_p_table\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: The Reckoning\n",
    "\n",
    "This is where we'll put the code to:\n",
    "\n",
    "* Fetch issues from each villain\n",
    "* Check whether valid: \n",
    "    * Check i_white/i_black, v_white/v_black\n",
    "    * Check if in spiderman_ids and vol[id]==31\n",
    "    * Flag the probability it's a Trade Paperback -- make a hash table\n",
    "* Create list of the json objects for each villain's issues, periodically export to file\n",
    "* Create table from json array, limiting to the fields we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mysterio', '4333', '466', \"'http://comicvine.gamespot.com/mysterio/4005-4333/'\"]\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "true_v = load_file(\"major_villains2.txt\",\"arr2\")\n",
    "print true_v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "spider_ids = load_file(\"spider_issue_ids.txt\",\"int\")\n",
    "spiderman = 1443\n",
    "random_spiders = numpy.random.choice(spider_ids,2000)\n",
    "vi_col = 1\n",
    "vn_col = 0\n",
    "\n",
    "i_black = load_file(\"i_black790\",'int')\n",
    "i_white = load_file(\"i_white790\",'int')\n",
    "v_black = load_file(\"v_black790\",'int')\n",
    "v_white = load_file(\"v_white790\",'int')\n",
    "\n",
    "resource_hash = {\"volume\":\"4050\",\"issue\":\"4000\",\"character\":\"4005\"}\n",
    "url_base = \"http://comicvine.gamespot.com/api/\"\n",
    "key = \"aff8790cd32512f45b429bb78cc21a7a87cf4d48\"\n",
    "\n",
    "actual_v_list = []\n",
    "v_issues_list = []\n",
    "\n",
    "def construct_url(i,resource):\n",
    "    new_base = url_base+resource+\"/\"+resource_hash[resource]+\"-\"+str(i)+\"/\"\n",
    "    target = new_base + \"?api_key=\"+key+\"&format=json\"\n",
    "    return target\n",
    "\n",
    "def url_to_data( url ):\n",
    "    req = urllib2.Request(url)\n",
    "    req.add_header('User-agent', 'Mozilla 5.10')\n",
    "    res = urllib2.urlopen(req)\n",
    "    data = json.load(res)['results']\n",
    "    return data\n",
    "\n",
    "def look_up(i,resource):\n",
    "    return url_to_data(construct_url(i,resource))\n",
    "\n",
    "def marvel(volume):\n",
    "    vi = volume['id']\n",
    "    if volume['publisher'] is not None:\n",
    "        if volume['publisher']['id'] == 31:\n",
    "            marvel_v.append(vi)\n",
    "            return True\n",
    "        else:\n",
    "            not_marvel.append(vi)\n",
    "            return False\n",
    "    else:\n",
    "        not_marvel.append(vi)\n",
    "        return False    \n",
    "\n",
    "def issue_list_from_vi(vi):\n",
    "    issues = look_up(vi, \"character\")[\"issue_credits\"]\n",
    "    return issues\n",
    "\n",
    "##\n",
    "def construct_issue_hash(vls):\n",
    "    v_issues_hash = {}\n",
    "    i = 0\n",
    "    for v in vls:\n",
    "        vi = v[vi_col]\n",
    "        vname = v[vn_col]\n",
    "        #print(\"adding \"+vname+\" as the \"+str(i)+\" villain\")\n",
    "        vissues = issue_list_from_vi(vi) # Why am I using name instead of id?\n",
    "        v_issues_hash[vname] = vissues\n",
    "        i = i+1\n",
    "    return v_issues_hash\n",
    "\n",
    "def p_original(ii,vol,table):\n",
    "    #Checking the white/blacklists should return over %100/0 probability -- a probability of 2 or -1 or something\n",
    "    if (ii in i_black):\n",
    "        print \"Black i\"\n",
    "        return -1\n",
    "    if (ii in i_white):\n",
    "        print \"White i\"\n",
    "        return 2\n",
    "    vi = vol[\"id\"]\n",
    "    if (vi in i_black):\n",
    "        print \"Black v\"\n",
    "        return -1\n",
    "    if (vi in i_white):\n",
    "        print \"White v\"\n",
    "        return 2\n",
    "    #Return the probability 0<p<1 this is an original\n",
    "    return p_vol(vol)\n",
    "    \n",
    "def p_vol(vol): \n",
    "    feats = volume_features(vol)\n",
    "    p = prob_of_label(feats,classifier)\n",
    "    return p[\"orig\"]\n",
    "\n",
    "def write_file(ls,filename):\n",
    "    f = open(filename, 'w')\n",
    "    for item in ls:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "def write_json_file(ls,filename):\n",
    "    f = open(filename,'w')\n",
    "    if type(ls) == dict:\n",
    "        print \"dumping dict!\"+str(len(ls))\n",
    "        d = json.dumps(ls)\n",
    "        print len(d)\n",
    "        f.write(d)\n",
    "    else:\n",
    "        json_string_arr = convert_to_star(ls)\n",
    "        for item in json_string_arr:\n",
    "            f.write(\"%s\\n\" % item)        \n",
    "        \n",
    "def convert_to_star(ls):\n",
    "    arr = []\n",
    "    for item in ls:\n",
    "        json_string = json.dumps(item)\n",
    "        arr.append(json_string)\n",
    "    return arr\n",
    "        \n",
    "def archive(ls,filen,num):\n",
    "    new_file = filen+str(num)\n",
    "    write_json_file(ls,new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36364, 2127, 18619, 6354, 6218, 27176, 57535, 9202, 3519, 73420, 18094, 7084, 43215, 78701, 77857, 28576, 29838, 32888, 11310, 2576]\n"
     ]
    }
   ],
   "source": [
    "marvel_v = load_file(\"marvel_v0\",\"int\")\n",
    "not_marvel = load_file(\"not_marvel0\",\"int\")\n",
    "\n",
    "print marvel_v[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## I think I should to make a list of issue_data for each villain in this, since I'm pulling the data here     \n",
    "def process_issues(villain_iss_id_dict):\n",
    "    p_table = dict(load_file(\"p_table1500\",\"json\")[0]) #replace with whatever the latest p_table archive is\n",
    "    len(p_table)\n",
    "    name_to_issue_data = {}\n",
    "    for vill in villain_iss_id_dict:\n",
    "        i = 0\n",
    "        #vill = \"Hobgoblin Kingsley\" # Trying to see if my blacklist/whitelist does anything\n",
    "        print \"Processing \"+vill+\" \"+str(i)\n",
    "        #if i>0: #Limits to the first villain, TAKE OUT when you know it works\n",
    "            #return {\"prob\": p_table, \"name_to_issue_data\": name_to_issue_data}\n",
    "        issues = villain_iss_id_dict[vill]\n",
    "        issue_data_ls = []\n",
    "        for issue in issues: ##TAKE OUT the 0:10\n",
    "            ## ARCHIVING p_table for ever 50 additions to p_table, issue_data for every 100 issues            \n",
    "            issue_data = look_up(issue[\"id\"],\"issue\")\n",
    "            vi = issue_data[\"volume\"][\"id\"]\n",
    "            if vi in marvel_v:\n",
    "                issue_data_ls.append(issue_data) ##I am now also worried I am doing this wronng...oh I totally am, move it outsifde the if stat\n",
    "                if str(vi) not in p_table:\n",
    "                    time.sleep(1)\n",
    "                    vol_data = look_up(vi,\"volume\")\n",
    "                    prob = p_original(issue,vol_data,p_table)\n",
    "#                    print \"Adding to p_table\"\n",
    "                    p_table[vi] = prob ## add to p_table\n",
    "            else:\n",
    "                if vi not in not_marvel:\n",
    "                    time.sleep(1)\n",
    "                    vol_data = look_up(vi, \"volume\")\n",
    "                    if marvel(vol_data):\n",
    "                        issue_data_ls.append(issue_data)\n",
    "                        if str(vi) not in p_table:\n",
    "                            prob = p_original(issue,vol_data,p_table)\n",
    "#                            print \"Adding to p_table\"\n",
    "                            p_table[vi] = prob\n",
    "#                    else:\n",
    "#                        print str(vi)+\" evaluated, and not marvel!\"\n",
    "#                else:\n",
    "#                    print str(vi)+\" in the not-marvel list!\"\n",
    "                \n",
    "            i = i+1\n",
    "            time.sleep(1)\n",
    "            if (i%100 == 0) or (i == len(issues)-1):\n",
    "                print vill+str(i)\n",
    "                archive(issue_data_ls,vill,i)\n",
    "            if ((len(p_table)%50 == 0) & (len(p_table)!=1202)) or (i == len(issues)-1):\n",
    "                print \"ptable\"+str(len(p_table))\n",
    "                archive(p_table,\"p_table\",len(p_table))\n",
    "        name_to_issue_data[vill] = issue_data_ls\n",
    "        holder_dict = dict(name_to_issue_data)\n",
    "        archive(holder_dict,\"holder_dict\",0)\n",
    "        archive(marvel_v,\"marvel_v\",0)\n",
    "        archive(not_marvel,\"not_marvel\",0)\n",
    "        \n",
    "    return {\"prob\": p_table, \"name_to_issue_data\": name_to_issue_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_to_issue_ids = construct_issue_hash(true_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = load_file(\"holder_dict0\",\"json\")\n",
    "b = load_file(\"holder_dict1\",\"json\")\n",
    "c = load_file(\"holder_dict2\",\"json\")\n",
    "d = load_file(\"holder_dict3\",\"json\")\n",
    "e = load_file(\"pre_processed_dict0\",\"json\")\n",
    "\n",
    "a[0].update(b[0])\n",
    "a[0].update(c[0])\n",
    "a[0].update(d[0])\n",
    "a[0].update(e[0])\n",
    "\n",
    "#a[0].update(pre_processed_villains)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregate_iv = a[0]\n",
    "vi_to_prob = dict(load_file(\"juked_p_table0\",\"json\")[0])#load_file(\"with_preloads0\",\"json\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8359\n"
     ]
    }
   ],
   "source": [
    "nrows=0\n",
    "for villain in aggregate_iv:\n",
    "    nrows = nrows + len(aggregate_iv[villain])\n",
    "    \n",
    "print nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1630\n",
      "0.511901349485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([669,  41,  21,  36,  12,  31,  25,  34,  56, 699]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = []\n",
    "for vol in vi_to_prob:\n",
    "    probs.append(vi_to_prob[vol])\n",
    "    \n",
    "print len(probs)\n",
    "print numpy.mean(probs)\n",
    "numpy.histogram(probs,range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_csv(ls,filename,delim):\n",
    "    f = open(filename, 'w')\n",
    "    for item in ls:\n",
    "#        print item\n",
    "        row = \"\"\n",
    "        j = 0\n",
    "        for i in item:\n",
    "            if (j == len(item)-1):\n",
    "                row = row+str(i)\n",
    "            else:\n",
    "                row=row+str(i)+delim\n",
    "                j = j+1\n",
    "        f.write(\"%s\\n\" % row)\n",
    "\n",
    "test_csv = [['this','is','a','test','csv'],[1,2,3,4,5]]\n",
    "write_csv(test_csv,\"test_csv\",';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "done = []\n",
    "\n",
    "def build_csv(iv_dict):\n",
    "    csv_arr = [[\"villain\", \"prob_orig\", \"issue_date\", \"issue_id\", \"volume_title\", \"volume_id\", \"volume_url\"]]\n",
    "    for vill in iv_dict:\n",
    "        issues = iv_dict[vill]\n",
    "        #print issues[0]\n",
    "        for issue in issues:\n",
    "            if issue is not None:\n",
    "                row = construct_row(vill,issue)\n",
    "                csv_arr.append(row)\n",
    "        done.append(vill)\n",
    "        write_csv(csv_arr,\"csv_new_classifier\",\";\")\n",
    "    return csv_arr\n",
    "            \n",
    "def construct_row(v_name,i_data):\n",
    "    villain = v_name\n",
    "    #print type(i_data)\n",
    "    issue_id = i_data[\"id\"]\n",
    "    volume_id = i_data[\"volume\"][\"id\"]    \n",
    "    prob_orig = get_prob(volume_id)\n",
    "    issue_date = i_data[\"cover_date\"]\n",
    "    volume_title = strip_non_ascii(i_data[\"volume\"][\"name\"])\n",
    "    volume_url = i_data[\"volume\"][\"site_detail_url\"]\n",
    "    row = [villain, prob_orig, issue_date, issue_id, volume_title, volume_id, volume_url]\n",
    "    return row\n",
    "    \n",
    "def get_prob(vi):\n",
    "    if str(vi) in vi_to_prob:\n",
    "        return round(vi_to_prob[str(vi)],10)\n",
    "    else:\n",
    "        time.sleep(1)\n",
    "        vol_data = look_up(vi,\"volume\")\n",
    "        return p_vol(vol_data)\n",
    "\n",
    "#takes in an array, converts to a string        \n",
    "csv_arr = build_csv(aggregate_iv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8352786758\n",
      "http://comicvine.gamespot.com/marvel-team-up/4050-6011/\n",
      "0.8912025071\n",
      "http://comicvine.gamespot.com/allnew-marvel-now-previews/4050-70186/\n",
      "0.8128843533\n",
      "http://comicvine.gamespot.com/marvel-universe-vs-the-punisher/4050-34724/\n",
      "0.7553976258\n",
      "http://comicvine.gamespot.com/marvel-adventures-spider-man/4050-18094/\n",
      "0.703521925\n",
      "http://comicvine.gamespot.com/space-punisher/4050-50365/\n",
      "0.8352786758\n",
      "http://comicvine.gamespot.com/spider-woman/4050-9573/\n",
      "0.7912337079\n",
      "http://comicvine.gamespot.com/1872/4050-83118/\n",
      "0.7140886371\n",
      "http://comicvine.gamespot.com/webspinners-tales-of-spider-man/4050-9375/\n",
      "0.7977346998\n",
      "http://comicvine.gamespot.com/silver-sable-and-the-wild-pack/4050-4810/\n",
      "0.8232201232\n",
      "http://comicvine.gamespot.com/wolverine-old-man-logan/4050-31674/\n",
      "0.7226890538\n",
      "http://comicvine.gamespot.com/ultimate-six/4050-19405/\n",
      "0.8877754277\n",
      "http://comicvine.gamespot.com/official-handbook-of-the-ultimate-marvel-universe-/4050-65739/\n",
      "0.8833697134\n",
      "http://comicvine.gamespot.com/maximum-carnage/4050-27313/\n",
      "0.7912337079\n",
      "http://comicvine.gamespot.com/marvel-zombies/4050-82505/\n",
      "0.7776108602\n",
      "http://comicvine.gamespot.com/the-superior-foes-of-spider-man/4050-64687/\n",
      "0.7553976258\n",
      "http://comicvine.gamespot.com/marvel-adventures-spider-man/4050-18094/\n",
      "0.8343816286\n",
      "http://comicvine.gamespot.com/avengers-origins-scarlet-witch-quicksilver/4050-44148/\n",
      "0.7553976258\n",
      "http://comicvine.gamespot.com/marvel-adventures-spider-man/4050-18094/\n",
      "0.7776108602\n",
      "http://comicvine.gamespot.com/the-superior-foes-of-spider-man/4050-64687/\n",
      "0.7553976258\n",
      "http://comicvine.gamespot.com/marvel-adventures-spider-man/4050-18094/\n"
     ]
    }
   ],
   "source": [
    "filtered_csv_arr = []\n",
    "\n",
    "for row in csv_arr:\n",
    "    if (row[1]<1)&(row[1]>.7):\n",
    "        filtered_csv_arr.append(row)\n",
    "        \n",
    "\n",
    "data_size = len(filtered_csv_arr)\n",
    "numpy.random.seed(3)\n",
    "random_arr = numpy.random.choice(range(0,data_size),data_size)\n",
    "\n",
    "for r in random_arr[0:20]:\n",
    "    print filtered_csv_arr[r][1]\n",
    "    print filtered_csv_arr[r][6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "test_issue_dict = dict(name_to_issue_ids)\n",
    "\n",
    "a = load_file(\"holder_dict0\",\"json\")\n",
    "b = load_file(\"holder_dict1\",\"json\")\n",
    "c = load_file(\"holder_dict2\",\"json\")\n",
    "\n",
    "a[0].update(b[0])\n",
    "a[0].update(c[0])\n",
    "\n",
    "to_pop = []\n",
    "\n",
    "for i in a[0].keys():\n",
    "    to_pop.append(str(i))\n",
    "\n",
    "done = []\n",
    "    \n",
    "for villain in to_pop:\n",
    "    if villain in test_issue_dict.keys():\n",
    "        test_issue_dict.pop(villain)\n",
    "\n",
    "print len(to_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_issue_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7a456ecd1c40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malready_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtest_issue_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_issue_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_issue_dict' is not defined"
     ]
    }
   ],
   "source": [
    "already_done = [\"Kraven the Hunter\", \"Scorpion\", \"Mysterio\", \"Vulture\", \"Vulture Drago\", \"Hobgoblin Kingsley\",\n",
    "             \"Hobgoblin Macendale\", \"Hobgoblin 2211\", \"Francine Frye\", \"Shocker\"]\n",
    "\n",
    "#for v in already_done:\n",
    "#    test_issue_dict.pop(v)\n",
    "#print len(test_issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kraven the Hunter', 'Mysterio', 'Hobgoblin Macendale', 'Scorpion', 'Vulture', 'Hobgoblin 2211', 'Vulture Drago', 'Francine Frye', 'Hobgoblin Kingsley', 'Shocker']\n"
     ]
    }
   ],
   "source": [
    "already_done = [\"Kraven the Hunter\", \"Scorpion\", \"Mysterio\", \"Vulture\", \"Vulture Drago\", \"Hobgoblin Kingsley\",\n",
    "             \"Hobgoblin Macendale\", \"Hobgoblin 2211\", \"Francine Frye\", \"Shocker\"]\n",
    "\n",
    "numbers = [598,899,516,74,46,359,167,11,13,399]\n",
    "\n",
    "def process_pre_processed(ls):\n",
    "    pre_dict = {}\n",
    "    i = 0\n",
    "    for villain in ls:\n",
    "        issue_data = process_pre_str(villain,i)\n",
    "        issue_data.append(process_last_issues(villain, i))\n",
    "        pre_dict[villain] = issue_data\n",
    "        i = i+1\n",
    "    return pre_dict\n",
    "        \n",
    "\n",
    "def process_last_issues(v,arr_index):\n",
    "    p_table = dict(load_file(\"p_table1630\",\"json\")[0])\n",
    "    row_index = numbers[arr_index]\n",
    "    while row_index < len(name_to_issue_ids[v]):\n",
    "        issue = name_to_issue_ids[v][row_index]\n",
    "        ii = issue[\"id\"]\n",
    "        i_data = look_up(ii,\"issue\")\n",
    "        #print i_data\n",
    "        vi = i_data[\"volume\"][\"id\"]\n",
    "        if vi in marvel_v:\n",
    "            if str(vi) not in p_table:\n",
    "                time.sleep(1)\n",
    "                vol_data = look_up(vi,\"volume\")\n",
    "                prob = p_original(issue,vol_data,p_table)\n",
    "                p_table[vi] = prob\n",
    "                archive(p_table,\"with_preloads\",0)\n",
    "            return i_data\n",
    "        else:\n",
    "            if vi not in not_marvel:\n",
    "                time.sleep(1)\n",
    "                vol_data = look_up(vi, \"volume\")\n",
    "                if marvel(vol_data):\n",
    "                    if str(vi) not in p_table:\n",
    "                        prob = p_original(issue,vol_data,p_table)\n",
    "                        p_table[vi] = prob\n",
    "                        archive(p_table,\"with_preloads\",0)\n",
    "                    return i_data\n",
    "        row_index = row_index+1\n",
    "    \n",
    "    \n",
    "def process_pre_str(v,i):\n",
    "    issues = load_pre_villain(v,i)\n",
    "    issue_data = []\n",
    "    for issue in issues:\n",
    "        issue_data.append(issue)\n",
    "    return issue_data\n",
    "\n",
    "def get_issue_count(v):\n",
    "    return len(name_to_issue_ids[v])\n",
    "    \n",
    "def load_pre_villain(v,index):\n",
    "    i = numbers[index]\n",
    "    filename = v+str(i)\n",
    "    return load_file(filename,\"json\")\n",
    "\n",
    "pre_processed_villains = process_pre_processed(already_done)\n",
    "print pre_processed_villains.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumping dict!10\n",
      "49984691\n"
     ]
    }
   ],
   "source": [
    "archive(pre_processed_villains,\"pre_processed_dict\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Kingpin 0\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "ptable1500\n",
      "dumping dict!1500\n",
      "45117\n",
      "Kingpin100\n",
      "Kingpin200\n",
      "Kingpin300\n",
      "Kingpin400\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "ptable1550\n",
      "dumping dict!1550\n",
      "46656\n",
      "Kingpin500\n",
      "Kingpin600\n",
      "Kingpin700\n",
      "Kingpin800\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "ptable1600\n",
      "dumping dict!1600\n",
      "48163\n",
      "Kingpin900\n",
      "Kingpin1000\n",
      "Kingpin1100\n",
      "Kingpin1200\n",
      "Kingpin1300\n",
      "Kingpin1323\n",
      "ptable1630\n",
      "dumping dict!1630\n",
      "49020\n",
      "dumping dict!1\n",
      "18428782\n"
     ]
    }
   ],
   "source": [
    "#test_issue_dict = dict(name_to_issue_ids)\n",
    "#test_issue_dict.pop(\"Kraven the Hunter\")\n",
    "#test_issue_dict.pop(\"Scorpion\")\n",
    "#test_issue_dict.pop(\"Mysterio\")\n",
    "#test_issue_dict.pop(\"Vulture\")\n",
    "#test_issue_dict.pop(\"Vulture Drago\")\n",
    "#test_issue_dict.pop(\"Hobgoblin Kingsley\")\n",
    "#test_issue_dict.pop(\"Hobgoblin Macendale\")\n",
    "#test_issue_dict.pop(\"Hobgoblin 2211\")\n",
    "#test_issue_dict.pop(\"Francine Frye\")\n",
    "#test_issue_dict.pop(\"Shocker\")\n",
    "\n",
    "processed = process_issues(test_issue_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = load_file(\"test2\",\"json\")\n",
    "g = name_to_issue_ids[\"Hobgoblin 2211\"]\n",
    "\n",
    "hoohaw = process_issues({\"test\":g})\n",
    "print len(f)\n",
    "print len(g)\n",
    "print len(hoohaw[\"name_to_issue_data\"][\"test\"])\n",
    "\n",
    "original = set()\n",
    "print \"Original data...\"\n",
    "for i in g:\n",
    "    original.add(i[\"id\"])\n",
    "\n",
    "processed = set()\n",
    "print \"Processed data...\"\n",
    "for i in hoohaw[\"name_to_issue_data\"][\"test\"]:\n",
    "    processed.add(i[\"id\"])\n",
    "\n",
    "loaded = set()\n",
    "print \"Loaded file...\"\n",
    "for i in f:\n",
    "    loaded.add(i[\"id\"])\n",
    "    \n",
    "diff2 = original-processed    \n",
    "\n",
    "print diff2\n",
    "#print name_to_issue_ids[\"Hobgoblin 2211\"]\n",
    "print len(name_to_issue_ids)\n",
    "print len(test_issue_dict)\n",
    "        \n",
    "#ptable_and_vissues = process_issues(test_issue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n"
     ]
    }
   ],
   "source": [
    "p = load_file(\"p_table622\",\"json\")[0]\n",
    "print(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92833\n",
      "<p> Spanish trade collection of:</p><ul><li>Deadpool #1-4</li><li>Deadpool Vol. 2</li><li><a href=\"http://comicvine.gamespot.com/deadpool-kills-the-marvel-universe/4050-50940/\" data-ref-id=\"4050-50940\" rel=\"nofollow\">Deadpool Kills the Marvel Universe</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-killustrated/4050-56120/\" data-ref-id=\"4050-56120\" rel=\"nofollow\">Deadpool Killustrated</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-kills-deadpool/4050-64684/\" data-ref-id=\"4050-64684\" rel=\"nofollow\">Deadpool Kills Deadpool</a></li><li><a href=\"http://comicvine.gamespot.com/deadpool-vs-carnage/4050-72790/\" data-ref-id=\"4050-72790\" rel=\"nofollow\">Deadpool Vs. Carnage</a></li></ul><p>Published by the Spanish wing of Panini Comics.</p>\n",
      "0.44538006973\n"
     ]
    }
   ],
   "source": [
    "test_issue = name_to_issue_ids[\"Carnage\"][10]\n",
    "test_issue_data = look_up(test_issue[\"id\"],\"issue\")\n",
    "test_vi = test_issue_data[\"volume\"][\"id\"]\n",
    "test_volume = look_up(test_vi,\"volume\")\n",
    "\n",
    "p = p_original(test_issue[\"id\"],test_volume,table)\n",
    "if test_vi not in table:\n",
    "    table[test_vi] = p\n",
    "\n",
    "print test_volume[\"id\"]\n",
    "print test_volume[\"description\"]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n",
      "Black i\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "#God damn it I think the blacklists aren't working\n",
    "tt=[]\n",
    "\n",
    "for i in i_black[0:5]:\n",
    "    vi = look_up(i, \"issue\")[\"volume\"][\"id\"]\n",
    "    vol_data = look_up(vi, \"volume\")\n",
    "    print p_original(i,vol_data,tt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{92864: 0.013352561303844828, 92833: 0.4453800697303239, 85938: 0.9995698434914074, 95207: 5.032671507280794e-09}\n"
     ]
    }
   ],
   "source": [
    "print table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
